{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.653376"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gpt_params(seq_len, vocab_size, d_model, num_heads, num_layers):\n",
    "    \"\"\" Given GPT config calculate total number of parameters \"\"\"\n",
    "    ffw_size = 4*d_model # in GPT the number of intermediate features is always 4*d_model\n",
    "    # token and position embeddings\n",
    "    embeddings = d_model * vocab_size + d_model * seq_len\n",
    "    # transformer blocks\n",
    "    attention = 3*d_model**2 + 3*d_model # weights and biases\n",
    "    attproj = d_model**2 + d_model\n",
    "    ffw = d_model*(ffw_size) + ffw_size\n",
    "    ffwproj = ffw_size*d_model + d_model\n",
    "    layernorms = 2*2*d_model\n",
    "    # dense\n",
    "    ln_f = 2*d_model\n",
    "    dense = d_model*vocab_size # note: no bias here\n",
    "    # note: embeddings are not included in the param count!\n",
    "    total_params = num_layers*(attention + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "    return total_params\n",
    "\n",
    "gpt2 = dict(seq_len = 1024, vocab_size = 50257, d_model = 768, num_heads = 12, num_layers = 12)\n",
    "gpt_params(**gpt2)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinchilla_params(seq_len, vocab_size, d_model, num_heads, num_layers, ffw_size):\n",
    "    \"\"\" Parameters in the Chinchilla models. Unlike GPT they use relative positional embeddings. \"\"\"\n",
    "    # token embeddings only\n",
    "    embeddings = d_model * vocab_size\n",
    "    # transformer blocks\n",
    "    attention = 3*d_model**2 + 3*d_model # weights and biases\n",
    "    relative_pos = d_model**2 + 2*d_model # relative keys, content bias, relative bias\n",
    "    attproj = d_model**2 + d_model\n",
    "    ffw = d_model*ffw_size + ffw_size\n",
    "    ffwproj = ffw_size*d_model + d_model\n",
    "    layernorms = 2*2*d_model\n",
    "    # dense\n",
    "    ln_f = 2*d_model\n",
    "    dense = d_model*vocab_size # note: no bias here\n",
    "    # note: embeddings are not included in the param count!\n",
    "    total_params = num_layers*(attention + relative_pos + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44000000.0, 512, 2048, 64, 8, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in all the 50 Chinchilla models on the last page of the paper\n",
    "import json\n",
    "chinchilla_models_txt = '[[44000000.0, 512, 2048, 64, 8, 8], [57000000.0, 576, 2304, 64, 9, 9], [74000000.0, 640, 2560, 64, 10, 10], [90000000.0, 640, 2560, 64, 10, 13], [106000000.0, 640, 2560, 64, 10, 16], [117000000.0, 768, 3072, 64, 12, 12], [140000000.0, 768, 3072, 64, 12, 15], [163000000.0, 768, 3072, 64, 12, 18], [175000000.0, 896, 3584, 64, 14, 14], [196000000.0, 896, 3584, 64, 14, 16], [217000000.0, 896, 3584, 64, 14, 18], [251000000.0, 1024, 4096, 64, 16, 16], [278000000.0, 1024, 4096, 64, 16, 18], [306000000.0, 1024, 4096, 64, 16, 20], [425000000.0, 1280, 5120, 128, 10, 18], [489000000.0, 1280, 5120, 128, 10, 21], [509000000.0, 1408, 5632, 128, 11, 18], [552000000.0, 1280, 5120, 128, 10, 24], [587000000.0, 1408, 5632, 128, 11, 21], [632000000.0, 1536, 6144, 128, 12, 19], [664000000.0, 1408, 5632, 128, 11, 24], [724000000.0, 1536, 6144, 128, 12, 22], [816000000.0, 1536, 6144, 128, 12, 25], [893000000.0, 1792, 7168, 128, 14, 20], [1018000000.0, 1792, 7168, 128, 14, 23], [1143000000.0, 1792, 7168, 128, 14, 26], [1266000000.0, 2048, 8192, 128, 16, 22], [1424000000.0, 2176, 8704, 128, 17, 22], [1429000000.0, 2048, 8192, 128, 16, 25], [1593000000.0, 2048, 8192, 128, 16, 28], [1609000000.0, 2176, 8704, 128, 17, 25], [1731000000.0, 2304, 9216, 128, 18, 24], [1794000000.0, 2176, 8704, 128, 17, 28], [2007000000.0, 2304, 9216, 128, 18, 28], [2283000000.0, 2304, 9216, 128, 18, 32], [2298000000.0, 2560, 10240, 128, 20, 26], [2639000000.0, 2560, 10240, 128, 20, 30], [2980000000.0, 2560, 10240, 128, 20, 34], [3530000000.0, 2688, 10752, 128, 22, 36], [3802000000.0, 2816, 11264, 128, 22, 36], [4084000000.0, 2944, 11776, 128, 22, 36], [4516000000.0, 3072, 12288, 128, 24, 36], [6796000000.0, 3584, 14336, 128, 28, 40], [9293000000.0, 4096, 16384, 128, 32, 42], [11452000000.0, 4352, 17408, 128, 32, 47], [12295000000.0, 4608, 18432, 128, 36, 44], [12569000000.0, 4608, 18432, 128, 32, 47], [13735000000.0, 4864, 19456, 128, 32, 47], [14940000000.0, 4992, 19968, 128, 32, 49], [16183000000.0, 5120, 20480, 128, 40, 47]]'\n",
    "chilchilla_models = json.loads(chinchilla_models_txt) # all 50 models\n",
    "chilchilla_models[0] # tuples of params, d_model, ffw_size, kv_size, n_heads, n_layers from Table A9\n",
    "[44000000.0, 512, 2048, 64, 8, 8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
